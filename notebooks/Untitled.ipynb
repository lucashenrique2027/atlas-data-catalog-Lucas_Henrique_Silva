{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "54de6ea5-ee19-42be-b1d8-451c4aff4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "ATLAS_CONFIG = {\n",
    "    \"url\": \"http://localhost:21000\",\n",
    "    \"username\": \"admin\", \n",
    "    \"password\": \"admin\"\n",
    "}\n",
    "\n",
    "POSTGRES_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 2001,\n",
    "    \"database\": \"northwind\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0522f1f4-c861-4076-b940-82bd4e9cfee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# atlas_client.py\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "class AtlasClient:\n",
    "    \"\"\"Cliente para interagir com a API REST do Apache Atlas.\"\"\"\n",
    "    \n",
    "    def __init__(self, url, username, password):\n",
    "        self.base_url = f\"{url}/api/atlas/v2\"\n",
    "        self.auth = HTTPBasicAuth(username, password)\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "        print(f\"AtlasClient inicializado para {self.base_url}\")\n",
    "\n",
    "    def _make_request(self, method, endpoint, **kwargs):\n",
    "        \"\"\"Método helper para fazer requisições e tratar erros.\"\"\"\n",
    "        try:\n",
    "            url = f\"{self.base_url}{endpoint}\"\n",
    "            response = requests.request(method, url, auth=self.auth, headers=self.headers, **kwargs)\n",
    "            response.raise_for_status()  \n",
    "            return response.json()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"Erro HTTP {e.response.status_code} no endpoint {endpoint}: {e.response.text}\")\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erro de conexão com Atlas: {e}\")\n",
    "            return None\n",
    "            \n",
    "    \n",
    "    def search_entities(self, query):\n",
    "        \"\"\"Busca entidades usando a busca básica (GET /search/basic).\"\"\"\n",
    "        endpoint = f\"/search/basic?query={query}&limit=10\"\n",
    "        return self._make_request(\"GET\", endpoint)\n",
    "\n",
    "    def create_entity(self, entity_data):\n",
    "        \"\"\"Cria uma ou mais entidades (POST /entity/bulk).\"\"\"\n",
    "        endpoint = \"/entity/bulk\"\n",
    "        payload = {\"entities\": entity_data if isinstance(entity_data, list) else [entity_data]}\n",
    "        return self._make_request(\"POST\", endpoint, data=json.dumps(payload))\n",
    "        \n",
    "    def get_entity(self, guid):\n",
    "        \"\"\"Obtém uma entidade pelo seu GUID (GET /entity/guid/{guid}).\"\"\"\n",
    "        endpoint = f\"/entity/guid/{guid}\"\n",
    "        return self._make_request(\"GET\", endpoint)\n",
    "    \n",
    "    def get_lineage(self, guid):\n",
    "        \"\"\"Obtém a linhagem de uma entidade (GET /lineage/{guid}).\"\"\"\n",
    "        endpoint = f\"/lineage/{guid}\"\n",
    "        return self._make_request(\"GET\", endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8cad6c7d-c89f-4d60-a7d7-12e2282eda28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# postgres_extractor.py\n",
    "import sys\n",
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "class PostgreSQLExtractor:\n",
    "    \"\"\"Extrai metadados de tabelas do banco de dados PostgreSQL Northwind.\"\"\"\n",
    "\n",
    "    def __init__(self, host, port, database, user, password):\n",
    "        self.conn_params = f\"host='{host}' port='{port}' dbname='{database}' user='{user}' password='{password}'\"\n",
    "        self.database_name = database\n",
    "        \n",
    "    def _execute_query(self, query):\n",
    "        \"\"\"Conecta e executa uma query, retornando resultados como lista de tuplas.\"\"\"\n",
    "        conn = None\n",
    "        try:\n",
    "            conn = psycopg2.connect(self.conn_params)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(query)\n",
    "            return cur.fetchall()\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao conectar ou executar query: {e}\")\n",
    "            return []\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "\n",
    "    def extract_table_and_column_metadata(self, schema='public'):\n",
    "        \"\"\"Extrai nome da tabela, colunas (nome, tipo, nullable, posição) e PKs.\"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            c.table_name, \n",
    "            c.column_name, \n",
    "            c.data_type, \n",
    "            CASE WHEN c.is_nullable = 'YES' THEN TRUE ELSE FALSE END AS is_nullable,\n",
    "            c.ordinal_position,\n",
    "            CASE WHEN tco.constraint_type = 'PRIMARY KEY' THEN TRUE ELSE FALSE END AS is_pk\n",
    "        FROM \n",
    "            information_schema.columns c\n",
    "        LEFT JOIN \n",
    "            information_schema.key_column_usage kcu \n",
    "            ON c.table_schema = kcu.table_schema \n",
    "            AND c.table_name = kcu.table_name \n",
    "            AND c.column_name = kcu.column_name\n",
    "        LEFT JOIN \n",
    "            information_schema.table_constraints tco \n",
    "            ON kcu.constraint_name = tco.constraint_name \n",
    "            AND kcu.table_schema = tco.table_schema \n",
    "            AND kcu.table_name = tco.table_name\n",
    "        WHERE \n",
    "            c.table_schema = '{schema}'\n",
    "        ORDER BY \n",
    "            c.table_name, c.ordinal_position;\n",
    "        \"\"\"\n",
    "        results = self._execute_query(query)\n",
    "        columns = ['table_name', 'column_name', 'data_type', 'is_nullable', 'position', 'is_pk']\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "        \n",
    "        # Estrutura os dados por tabela\n",
    "        metadata = {}\n",
    "        for table_name, group in df.groupby('table_name'):\n",
    "            metadata[table_name] = {\n",
    "                'database': self.database_name,\n",
    "                'columns': group[['column_name', 'data_type', 'is_nullable', 'position', 'is_pk']].to_dict('records')\n",
    "            }\n",
    "        return metadata\n",
    "\n",
    "    def extract_relationships(self, schema='public'):\n",
    "        \"\"\"Extrai relacionamentos (Foreign Keys) para linhagem.\"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            tc.table_name AS from_table,\n",
    "            kcu.column_name AS from_column,\n",
    "            ccu.table_name AS to_table,\n",
    "            ccu.column_name AS to_column\n",
    "        FROM \n",
    "            information_schema.table_constraints AS tc \n",
    "        JOIN \n",
    "            information_schema.key_column_usage AS kcu\n",
    "            ON tc.constraint_name = kcu.constraint_name\n",
    "        JOIN \n",
    "            information_schema.constraint_column_usage AS ccu\n",
    "            ON ccu.constraint_name = tc.constraint_name\n",
    "        WHERE \n",
    "            tc.constraint_type = 'FOREIGN KEY' AND tc.table_schema = '{schema}';\n",
    "        \"\"\"\n",
    "        results = self._execute_query(query)\n",
    "        columns = ['from_table', 'from_column', 'to_table', 'to_column']\n",
    "        return pd.DataFrame(results, columns=columns).to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3970aa43-cf24-4a1a-9f00-e8258f99a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_catalogger.py\n",
    "class DataCatalogger:\n",
    "    \"\"\"Gerencia a catalogação de metadados no Apache Atlas.\"\"\"\n",
    "    \n",
    "    def __init__(self, atlas_client, extractor):\n",
    "        self.atlas = atlas_client\n",
    "        self.extractor = extractor\n",
    "        self.cluster_name = \"cluster1\" \n",
    "        self.database_entity_guid = None\n",
    "        self.table_guids = {} \n",
    "\n",
    "    def _get_qualified_name(self, entity_type, **kwargs):\n",
    "        \"\"\"Gera o qualifiedName (QN) único para entidades do Atlas.\"\"\"\n",
    "        db_name = self.extractor.database_name\n",
    "        \n",
    "        if entity_type == \"hive_db\":\n",
    "            return f\"{db_name}@{self.cluster_name}\"\n",
    "        elif entity_type == \"hive_table\":\n",
    "            table_name = kwargs['table_name']\n",
    "            return f\"{db_name}.{table_name}@{self.cluster_name}\"\n",
    "        elif entity_type == \"hive_column\":\n",
    "            table_name = kwargs['table_name']\n",
    "            column_name = kwargs['column_name']\n",
    "            return f\"{db_name}.{table_name}.{column_name}@{self.cluster_name}\"\n",
    "        return None\n",
    "\n",
    "    def _create_database_entity(self):\n",
    "        \"\"\"Cria ou obtém o GUID da entidade Database (northwind_postgres).\"\"\"\n",
    "        db_qn = self._get_qualified_name(\"hive_db\")\n",
    "        \n",
    "        # 1. Tentar buscar se já existe (simplificado)\n",
    "        search_result = self.atlas.search_entities(db_qn)\n",
    "        if search_result and search_result.get('entities'):\n",
    "            self.database_entity_guid = search_result['entities'][0]['guid']\n",
    "            print(f\"Database '{db_qn}' já catalogada. GUID: {self.database_entity_guid}\")\n",
    "            return\n",
    "\n",
    "        # 2. Criar se não existir\n",
    "        db_entity = {\n",
    "            \"typeName\": \"hive_db\",\n",
    "            \"attributes\": {\n",
    "                \"name\": self.extractor.database_name,\n",
    "                \"qualifiedName\": db_qn,\n",
    "                \"clusterName\": self.cluster_name,\n",
    "                \"owner\": \"postgres\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"Catalogando Database: {self.extractor.database_name}\")\n",
    "        response = self.atlas.create_entity(db_entity)\n",
    "        if response and response.get('entities'):\n",
    "            self.database_entity_guid = response['entities']['hive_db'][0]['guid']\n",
    "            print(f\"Database catalogada com sucesso. GUID: {self.database_entity_guid}\")\n",
    "            \n",
    "    def _create_table_and_column_entities(self, table_name, table_data):\n",
    "        \"\"\"Cria as entidades Tabela e suas Colunas filhas no Atlas.\"\"\"\n",
    "        if not self.database_entity_guid:\n",
    "            print(\"Erro: Database GUID não encontrado.\")\n",
    "            return\n",
    "\n",
    "        column_entities = []\n",
    "        for col_data in table_data['columns']:\n",
    "            col_qn = self._get_qualified_name(\"hive_column\", table_name=table_name, column_name=col_data['column_name'])\n",
    "            column_entities.append({\n",
    "                \"typeName\": \"hive_column\",\n",
    "                \"attributes\": {\n",
    "                    \"name\": col_data['column_name'],\n",
    "                    \"qualifiedName\": col_qn,\n",
    "                    \"type\": col_data['data_type'],\n",
    "                    \"position\": col_data['position'],\n",
    "                    \"isPrimaryKey\": col_data['is_pk'],\n",
    "                    \"isNullable\": col_data['is_nullable']\n",
    "                },\n",
    "                \"status\": \"ACTIVE\",\n",
    "                \"relationshipAttributes\": {\n",
    "                    \"table\": {\"typeName\": \"hive_table\", \"uniqueAttributes\": {\"qualifiedName\": self._get_qualified_name(\"hive_table\", table_name=table_name)}}\n",
    "                }\n",
    "            })\n",
    "            \n",
    "        table_qn = self._get_qualified_name(\"hive_table\", table_name=table_name)\n",
    "        table_entity = {\n",
    "            \"typeName\": \"hive_table\",\n",
    "            \"attributes\": {\n",
    "                \"name\": table_name,\n",
    "                \"qualifiedName\": table_qn,\n",
    "                \"owner\": \"postgres\",\n",
    "                \"description\": f\"Tabela '{table_name}' do banco Northwind.\",\n",
    "                \"db\": {\"guid\": self.database_entity_guid}, \n",
    "                \"columns\": [{\"typeName\": \"hive_column\", \"uniqueAttributes\": {\"qualifiedName\": c['attributes']['qualifiedName']}} for c in column_entities]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        entities_to_create = [table_entity] + column_entities\n",
    "        print(f\"Catalogando Tabela: {table_name} e {len(column_entities)} colunas...\")\n",
    "        response = self.atlas.create_entity(entities_to_create)\n",
    "        \n",
    "        if response and response.get('entities'):\n",
    "            guid = response['entities']['hive_table'][0]['guid']\n",
    "            self.table_guids[table_name] = guid\n",
    "            print(f\"Tabela '{table_name}' catalogada. GUID: {guid}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Falha ao catalogar tabela {table_name}.\")\n",
    "            return False\n",
    "\n",
    "    def _create_lineage_entities(self, relationships):\n",
    "        \"\"\"Cria entidades de Processo para representar a linhagem via FKs.\"\"\"\n",
    "        \n",
    "       \n",
    "        processes_created = 0\n",
    "        \n",
    "        for rel in relationships:\n",
    "            from_table = rel['from_table']\n",
    "            to_table = rel['to_table']\n",
    "            \n",
    "            if from_table not in self.table_guids or to_table not in self.table_guids:\n",
    "                print(f\"Pulando linhagem: Tabela não catalogada ({from_table} -> {to_table})\")\n",
    "                continue\n",
    "\n",
    "            process_name = f\"fk_link_{from_table}_to_{to_table}\"\n",
    "            process_qn = f\"{process_name}@{self.cluster_name}\"\n",
    "            \n",
    "            process_entity = {\n",
    "                \"typeName\": \"Process\", # Tipo base que suporta inputs/outputs\n",
    "                \"attributes\": {\n",
    "                    \"name\": process_name,\n",
    "                    \"qualifiedName\": process_qn,\n",
    "                    \"description\": f\"Relacionamento FK de {from_table} para {to_table}\",\n",
    "                    \"inputs\": [\n",
    "                        {\"guid\": self.table_guids[from_table]} # Tabela de onde vem o dado (from)\n",
    "                    ],\n",
    "                    \"outputs\": [\n",
    "                        {\"guid\": self.table_guids[to_table]} # Tabela para onde vai o dado (to)\n",
    "                    ],\n",
    "                    \"operationType\": \"FK_RELATIONSHIP\", \n",
    "                    \"owner\": \"system\"\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = self.atlas.create_entity(process_entity)\n",
    "            if response and response.get('entities'):\n",
    "                processes_created += 1\n",
    "            else:\n",
    "                print(f\"Falha ao criar linhagem para {from_table} -> {to_table}\")\n",
    "                \n",
    "        print(f\"Criados {processes_created} processos de linhagem (FKs).\")\n",
    "        return processes_created\n",
    "\n",
    "    def catalog_all_tables(self):\n",
    "        \"\"\"Executa o pipeline completo de extração e catalogação.\"\"\"\n",
    "        \n",
    "        self._create_database_entity()\n",
    "        if not self.database_entity_guid:\n",
    "            return {\"status\": \"Erro\", \"message\": \"Falha ao catalogar Database.\"}\n",
    "        \n",
    "        table_metadata = self.extractor.extract_table_and_column_metadata()\n",
    "        \n",
    "        tables_created = 0\n",
    "        for table_name, data in table_metadata.items():\n",
    "            if self._create_table_and_column_entities(table_name, data):\n",
    "                tables_created += 1\n",
    "\n",
    "        relationships = self.extractor.extract_relationships()\n",
    "        processes_created = self._create_lineage_entities(relationships)\n",
    "\n",
    "        return {\n",
    "            \"status\": \"Sucesso\", \n",
    "            \"tables_created\": tables_created, \n",
    "            \"lineage_processes\": processes_created\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bbea85a2-adcb-478c-9898-2cf6a4cef0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discovery_report.py\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "class DiscoveryReport:\n",
    "    \"\"\"Gera um relatório das entidades catalogadas no Atlas.\"\"\"\n",
    "    \n",
    "    def __init__(self, atlas_client):\n",
    "        self.atlas = atlas_client\n",
    "        self.report_data = {}\n",
    "\n",
    "    def _get_all_entities_data(self):\n",
    "        \"\"\"Busca todas as entidades catalogadas do tipo hive_table e hive_db.\"\"\"\n",
    "        query = \"hive_db OR hive_table OR Process\" \n",
    "        search_result = self.atlas.search_entities(query)\n",
    "        \n",
    "        if not search_result or not search_result.get('entities'):\n",
    "            return []\n",
    "            \n",
    "        guids = [e['guid'] for e in search_result['entities']]\n",
    "        \n",
    "        # Para um relatório real, o ideal seria usar a API /entity/bulk/list?guid=...\n",
    "        # Simplificando, vamos processar a lista da busca e fazer contagens.\n",
    "        return search_result['entities']\n",
    "\n",
    "    def generate_report(self, filename_base):\n",
    "        \"\"\"Gera o relatório, calcula estatísticas e exporta para JSON/CSV.\"\"\"\n",
    "        entities = self._get_all_entities_data()\n",
    "        \n",
    "        # Estatísticas\n",
    "        stats = defaultdict(int)\n",
    "        table_column_counts = {}\n",
    "        relationships_count = 0\n",
    "        \n",
    "        for entity in entities:\n",
    "            type_name = entity.get('typeName')\n",
    "            stats[type_name] += 1\n",
    "            \n",
    "            if type_name == 'hive_table':\n",
    "              # stats['total_columns'] += entity['attributes'].get('numCols', 0)\n",
    "                pass\n",
    "            \n",
    "            elif type_name == 'Process':\n",
    "                relationships_count += 1 # Cada Processo criado representa uma FK/Linhagem\n",
    "        \n",
    "        stats['total_columns'] = stats.get('hive_column', 0)\n",
    "        \n",
    "        self.report_data = {\n",
    "            \"summary\": {\n",
    "                \"total_databases\": stats['hive_db'],\n",
    "                \"total_tables\": stats['hive_table'],\n",
    "                \"total_columns\": stats['total_columns'],\n",
    "                \"total_relationships\": relationships_count,\n",
    "            },\n",
    "            \"tables_by_type\": dict(stats)\n",
    "               }\n",
    "        \n",
    "        \n",
    "        self._export_json(f\"{filename_base}.json\")\n",
    "        self._export_csv(f\"{filename_base}.csv\")\n",
    "\n",
    "    def _export_json(self, filename):\n",
    "        \"\"\"Exporta o relatório para JSON.\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.report_data, f, indent=4)\n",
    "        print(f\"Relatório JSON exportado para {filename}\")\n",
    "\n",
    "    def _export_csv(self, filename):\n",
    "        \"\"\"Exporta o resumo principal para CSV.\"\"\"\n",
    "        summary = self.report_data.get('summary', {})\n",
    "        \n",
    "        \n",
    "        df_summary = pd.DataFrame(summary.items(), columns=['Métrica', 'Valor'])\n",
    "        \n",
    "        df_summary.to_csv(filename, index=False)\n",
    "        print(f\"Relatório CSV exportado para {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "304e851c-1b95-4bed-ab94-8bbb47258e1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jovyan/src\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ATLAS_CONFIG\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Inicializando o Catálogo de Dados Apache Atlas ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jovyan/src\")\n",
    "from config import ATLAS_CONFIG\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"--- Inicializando o Catálogo de Dados Apache Atlas ---\")\n",
    "    \n",
    "    \n",
    "    print(\"Aguardando 10 segundos para garantir que o Atlas esteja online...\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    try:\n",
    "        atlas = AtlasClient(**ATLAS_CONFIG)\n",
    "        extractor = PostgreSQLExtractor(**POSTGRES_CONFIG)\n",
    "        catalogger = DataCatalogger(atlas, extractor)\n",
    "        \n",
    "        print(\"\\n--- Iniciando catalogação de Database, Tabelas e Colunas ---\")\n",
    "        results = catalogger.catalog_all_tables()\n",
    "        print(f\"\\nStatus: {results['status']}\")\n",
    "        print(f\"{results['tables_created']} tabelas catalogadas\")\n",
    "        print(f\"{results['lineage_processes']} processos de linhagem criados\")\n",
    "        \n",
    "        print(\"\\n--- Gerando Relatório de Descoberta ---\")\n",
    "        report = DiscoveryReport(atlas)\n",
    "        time.sleep(5)\n",
    "        report.generate_report(\"discovery_report\")\n",
    "        print(\"Relatório gerado com sucesso!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nOcorreu um erro fatal no processo: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d30139-43c7-407e-9ab1-f2b11f5abe9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df261249-a384-4506-a6fc-e83781937afd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
