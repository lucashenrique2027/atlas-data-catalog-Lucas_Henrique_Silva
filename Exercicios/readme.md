## üìù README.md para `meu-catalogo-atlas`

Aqui est√° um modelo completo de `README.md` que cobre todos os requisitos do exerc√≠cio (Documenta√ß√£o: 20%) e o estrutura para a entrega.

-----

# üìö Cat√°logo de Dados PostgreSQL com Apache Atlas

## Vis√£o Geral do Projeto

Este projeto implementa um **Data Catalogger** autom√°tico para extrair metadados de um banco de dados **PostgreSQL** (Northwind) e catalog√°-los no **Apache Atlas** utilizando sua API REST. O objetivo principal √© automatizar a descoberta de metadados, a cria√ß√£o de entidades (Databases, Tabelas, Colunas) e a implementa√ß√£o de **Data Lineage** (Linhagem de Dados) baseada em chaves estrangeiras (Foreign Keys).

Este trabalho cumpre as Tarefas 1 a 4 do exerc√≠cio pr√°tico de DataOps e cataloga√ß√£o de metadados.

## üõ†Ô∏è Componentes e Tecnologias

| Componente | Fun√ß√£o |
| :--- | :--- |
| **Apache Atlas** | Cat√°logo de Dados e Reposit√≥rio de Metadados (via API REST) |
| **PostgreSQL** | Fonte de dados (Banco Northwind) |
| **Python** | Linguagem de script principal |
| `requests` | Comunica√ß√£o com a API REST do Atlas |
| `psycopg2` | Conex√£o e extra√ß√£o de metadados do PostgreSQL |
| `pandas` | Estrutura√ß√£o e manipula√ß√£o de metadados para relat√≥rios |

-----

## üöÄ Como Executar o Projeto

### Pr√©-requisitos

1.  **Docker** e **Docker Compose** instalados.
2.  Acesso ao reposit√≥rio base do laborat√≥rio para inicializa√ß√£o do ambiente Docker.

### 1\. Inicializa√ß√£o do Ambiente

O ambiente √© iniciado via `docker-compose.yml` contido no reposit√≥rio base.

```bash
# Navegue at√© o diret√≥rio que cont√©m o docker-compose.yml
docker-compose up -d

# Aguarde 5-10 minutos pela inicializa√ß√£o completa do Apache Atlas
docker-compose logs -f atlas

# Verifique os servi√ßos:
# Atlas: http://localhost:21000 (admin/admin)
# PostgreSQL: localhost:2001
```

### 2\. Instala√ß√£o das Depend√™ncias

Instale as bibliotecas Python necess√°rias (listadas em `requirements.txt`).

```bash
pip install -r requirements.txt
```

### 3\. Execu√ß√£o do Pipeline

O script `main.py` executa o pipeline completo: inicializa clientes, extrai metadados, cataloga as entidades e gera a linhagem.

```bash
python main.py
```

O script ir√° imprimir logs de cada etapa de cataloga√ß√£o e, ao final, gerar o relat√≥rio de descoberta.

-----

## üìÇ Estrutura do Projeto

```
meu-catalogo-atlas/
‚îú‚îÄ‚îÄ README.md                 # Este arquivo
‚îú‚îÄ‚îÄ requirements.txt          # Depend√™ncias Python
‚îú‚îÄ‚îÄ config.py                 # Configura√ß√µes de acesso (ATLAS, POSTGRES)
‚îú‚îÄ‚îÄ atlas_client.py           # Tarefa 1: Implementa√ß√£o da API REST do Atlas
‚îú‚îÄ‚îÄ postgres_extractor.py     # Tarefa 2: Extra√ß√£o de metadados do PostgreSQL
‚îú‚îÄ‚îÄ data_catalogger.py        # Tarefa 3: L√≥gica de mapeamento e cataloga√ß√£o no Atlas
‚îú‚îÄ‚îÄ discovery_report.py       # Tarefa 4: Gera√ß√£o de relat√≥rios
‚îî‚îÄ‚îÄ main.py                   # Script principal de execu√ß√£o do pipeline
```

-----

## üìã Detalhamento das Tarefas Implementadas

### Tarefa 1: `AtlasClient` (`atlas_client.py`)

A classe `AtlasClient` gerencia a comunica√ß√£o com a API Atlas, implementando autentica√ß√£o **HTTP Basic** e m√©todos de baixo n√≠vel:

  * `_make_request()`: Lida com requisi√ß√µes e tratamento de erros HTTP (`4xx`/`5xx`).
  * `search_entities(query)`: Busca entidades por nome qualificado (`qualifiedName`).
  * `create_entity(entity_data)`: Cria entidades em **bulk** (lote) usando a API `/entity/bulk`.
  * `get_entity(guid)` e `get_lineage(guid)`: Implementados para futuras extens√µes ou relat√≥rios.

### Tarefa 2: `PostgreSQLExtractor` (`postgres_extractor.py`)

A classe se conecta ao PostgreSQL (`psycopg2`) e utiliza o `information_schema` para uma extra√ß√£o completa e estruturada:

  * `extract_table_and_column_metadata()`: Retorna o nome da tabela, nome da coluna, tipo de dados, nulidade e status de **Primary Key (PK)**.
  * `extract_relationships()`: Busca todas as chaves estrangeiras (Foreign Keys) para uso na linhagem.

### Tarefa 3: `DataCatalogger` (`data_catalogger.py`)

Esta √© a orquestradora que traduz os metadados do Postgres para o formato de entidade do Atlas (Tipos `hive_db`, `hive_table`, `hive_column` e `Process`):

  * `_get_qualified_name()`: Cria o identificador √∫nico e hier√°rquico exigido pelo Atlas (ex: `northwind.customers@cluster1`).
  * `_create_database_entity()`: Garante que a entidade `hive_db` existe no Atlas.
  * `_create_table_and_column_entities()`: Cria a entidade `hive_table` e todas as suas `hive_column`s filhas em uma √∫nica chamada **bulk** (lote).
  * `_create_lineage_entities()`: Cria uma entidade **Process** do Atlas para cada relacionamento de Foreign Key encontrado, modelando o fluxo de dados (input -\> process -\> output).

### Tarefa 4: `DiscoveryReport` (`discovery_report.py`)

A classe utiliza o `AtlasClient` para buscar as entidades catalogadas e gerar estat√≠sticas.

  * `generate_report(filename_base)`: Obt√©m o resumo de todas as entidades e exporta os dados de estat√≠sticas em **JSON** e **CSV**.

## üìä Relat√≥rio de Descoberta

Ap√≥s a execu√ß√£o, os seguintes arquivos ser√£o gerados no diret√≥rio raiz:

  * `discovery_report.json`
  * `discovery_report.csv`